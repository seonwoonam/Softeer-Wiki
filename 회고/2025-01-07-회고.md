# 2025-01-07 리뷰 및 회고

## 리뷰
### SQL 팀활동
팀원분들과 어려웠던 키워드를 뽑고, 각자 그 키워드에 해당하는 문제를 한 문제씩 직접 만들어 서로 공유하고 풀어보는 시간을 가졌다. 키워드는 EXIST, CASE, GROUP BY + HAVING 을 뽑았고, GROUP BY + HAVING 문제를 만들었다. 문제를 직접 만들고, 팀원분들이 만들어주신 문제들을 풀어보면서 어떤 조건을 주어졌을 때 어떤식으로 쿼리가 작동해야 할지, 어떤 키워드를 사용해야 할지 구체적으로 생각해볼 수 있게 되었다. 또 각각 키워드의 쓰임새에 대해 복습해볼 수 있게 되었다. 

문제에 대한 답을 생각해보면서 Subquery를 사용한 쿼리와, Join을 사용한 쿼리 중 어떤 것이 효율적일지도 생각해보게 되었다. 상황에 따라 Join을 사용하는 것이 좋을 때도 있고, 서브 쿼리를 사용하는 것이 좋을 때도 있다. 하지만 보통 서브쿼리는 가독성이 좋지만 성능이 조인에 비해 좋지 않다고 한다. 

### ETL 프로세스 구현하기
- pycountry 라이브러리를 사용하여 지역 Mapping을 하고 출력 기능을 구현하였다.
- datetime 라이브러리를 이용하여 Log를 찍는 방법을 수정해주었다.
- Raw data를 json 형식으로 저장하도록 수정하였다.
- transform한 데이터를 DB 파일로 저장되도록 Load를 구현하였다.
- SQL 쿼리문을 사용해서 화면 출력을 하도록 구현하였다. 
- transform을 구현하는 과정에서 Pandas에 대해 일부 새로운 것들을 배울 수 있었다.
    - apply함수 : 데이터 셋에 내가 구현한 연산 혹은 함수를 적용해야 할 때 사용
    - group by를 할 수 있다는 점

- Pandas를 사용하게 되면 데이터가 인메모리 방식으로 처리되기 때문에 속도가 빠르다고 한다. 하지만 그와 동시에 데이터의 용량이 너무 커져버려 메모리의 크기를 초과해버리면 out-of-memory가 뜬다고 한다. 이에 따라 대용량 데이터라면 Pandas로는 처리를 하지 못할 것이다. 대용량 데이터라면 PySpark를 사용할 수 있다고 한다. pandas는 전체 데이터를 메모리에서 저장해 활용하는 eager 방식인 반면에 pyspark는 일부 데이터만을 사용하는 lazy 방식이어서 위와 같은 문제를 해결할 수 있다고 한다.

##### 생각
미션에서 Extract한 데이터를 Transform 하지 않고, Raw data 상태에서 json 파일로 저장하는 부분이 있다. 이렇게 하는 이유는 데이터를 어떻게 바라보느냐에 따라 데이터의 type이 바뀌듯이, 어떻게 사용할지 정해지지 않았거나, 정형화된 데이터의 Queue의 함정에 빠지지 않기 위해서 Raw Data를 저장한다고 생각한다. 
미션 과정의 Extract에서 위키피디아 url을 통해 html을 불러온 후 beautifulsoup를 이용해서 GDP 테이블에서 나라이름, GDP, Year을 찾아 배열에 넣은 후 이를 json 으로 저장하고 있다. 하지만 이러한 과정이 진정한 raw_data를 저장하고 있는지에 대한 의구심이 생겼다. raw_data를 저장하는 이유는 정형화된 데이터의 Queue의 함정에 빠지지 않기 위해라는 생각을 하였는데, html에서 beautifulsoup을 이용해 데이터를 extract하는 과정 속에서 이미 원하는 컬럼을 정해 뽑는 과정이 정형화를 하고 있는 것 아닐까 라는 생각이 들었다. 


## 회고
### Keep
- SQL 공부를 할 때 서로 문제를 만들고 풀어보았는데, 다시 한번 키워드의 개념을 확실히 하고 활용해보는 경험을 하여서 SQL 공부에 효과적이었다고 생각들었다. 
- Pandas의 한계, 그에 따른 대안의 등장에 대해 공부해 나가면서 프레임워크 혹은 라이브러리가 왜 등장하게 되었는지 생각해볼 수 있게 되었고, 상황에 맞춰 활용할 수 있을 것 같다라는 생각을 하였다.
- 배운 내용을 미션에 최대한 활용할 수 있도록 노력하였다.(pandas dataframe, SQL)

### Problem
- 미션 요구사항에서 로깅과정에서 datetime 라이브러리를 사용해야 한다는 문구를 빠뜨려 logging 라이브러리를 이용하여 구현하였다가 추후 발견 후 수정해주었다.
- 사이즈가 큰 데이터가 들어왔을 상황을 고려하지 못하고 코드를 구현하였다.

### Try
- check 리스트를 만들어 Todo를 헷갈리거나, 빠뜨리지 않고 할 수 있도록 해야겠다. 
- 사이즈가 큰 데이터가 들어왔을 때 어떻게 처리할 것인지 생각하고 코드를 수정해야겠다.