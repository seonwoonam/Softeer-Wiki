# 2025-01-29 리뷰 및 회고

## 리뷰

### 미니 프로젝트
- 웹 스크래핑 하는 extractor를 구현하고 있다. 팀원분께서 웹 사이트를 동적 크롤링하는 방법 말고, 개발자 도구에 있는 Network 탭에서 데이터들을 불러오는 API를 찾은 후 해당 API를 이용해서 데이터를 찾는 방법을 공유해주셨는데, 일일이 태그를 찾지 않아도 되고 편리한 것 같다. 

### W4M1
- 파이값 구하기 어플리케이션을 완수한 후 csv 파일이 마스터 노드의 저장공간에 남지 않아있어, 오류가 생긴줄 알았고, 해당 데이터를 찾느라 오래걸렸었다. 클러스터 모드 별로 저장되는 특징이 달랐는데, yarn과 같은 RM을 사용했을 경우에는 HDFS와 같은 중앙 저장소에 저장이 되지만, standalone 방식일 경우에는 각 Executor가 자신의 디스크에 저장한다. 이로인해 마스터 노드에서는 csv파일을 찾을 수 없었고, 워커 노드에서 찾을 수 있었다.
- 7077 포트 : Spark Master Port
- 8080 포트 : Master Web UI

### W4M2
- 미션을 하면서 spark dataframe을 활용해 볼 수 있었다. 기존 Pandas dataframe과 유사한 부분도 많았지만 지원이 안되는 기능들도 꽤 있었어서 유의해서 사용해야 겠다는 생각이 들었다.

```python
    # table 형태로 데이터프레임의 모든 데이터 보여줌.
    df.show()

    # dataframe에서 특정 인덱스의 데이터 가져온다.
    df.take(index)

    # agg 집계 함수 적용하기
    # Spark는 lazy Execution 방식이므로, collect를 호출해야 연산이 수행됨.
    avg_trip_duration = df.agg({"trip_duration": "avg"}).collect()[0][0]
```

## 회고
### Keep
- Network 탭에서 데이터들을 불러오는 방법을 적용시켜 보는 등 계속해서 새로운 것을 배우면 시도해보려고 노력한다. 아직은 배우는 단계이기 때문에 다양한 것을 시도해보는 것도 중요한 것 같다.

### Problem
- Spark에 대한 이해도가 아직 부족한 것 같다. M1에서 어플리케이션이 어떤 식으로 돌아가는지 확실히 이해하지 못했다. 개념에 대한 복습이 필요하다.

### Try
- Spark 개념을 복습해서 이해도를 높여야겠다. 이해하고 사용해야 제대로 활용할 수 있을 것이다.