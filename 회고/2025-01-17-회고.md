# 2025-01-17 리뷰 및 회고

## 리뷰
### W2M5 : Word Cloud 팀활동

- 지난번에 유튜브 API를 활용하여 추출했던 데이터셋을 이번에는 sellenium을 이용해서 직접 스크래핑해서 저장하였다. 
- 잇섭 채널의 각각의 비디오들의 제목, 조회수, 좋아요, 댓글 수 등과 같은 데이터들을 불러와 raw data로 저장하였다.
- transform 하는 과정에도 약간의 변화를 주었다. corr함수를 이용하여 조회수, 좋아요 수, 댓글 수 간의 상관관계를 파악해보았다.
- 그 결과 조회수와, 좋아요 수는 상관관계가 0.7로 높은편이었고, 조회수와 댓글은 0.3 정도로 낮아서 선호, 비선호 컨텐츠를 정할 때 조회수와 좋아요 수만을 사용했다.
- 이를 이용해 조회수와 좋아요 수 모두 상위 25%인 영상을 선호 컨텐츠, 조회수와 좋아요 수 모두 하위 25%인 영상을 비선호 컨텐츠로 정의하였다.
- 실험 결과 해당 크리에이터의 영상 주제가 비슷하다보니 여전히, 조회수가 많은 word cloud와 적은 word cloud가 겹치는게 많은 것을 확인할 수 있었다. 그래도 조회수가 많은 컨텐츠로는 삼성 갤럭시 울트라, 키보드, 폴드, 일주일 사용기와 같은 키워드들이 눈에 띄게 보이는 것을 확인할 수 있었다. 
- 추출한 데이터의 전처리를 추가하여 더욱 보완할 수 있을 것 같다.
- 새롭게 구현한 Extract 과정 속에서는 프로그램을 돌리면 시간이 너무 오래 걸려 멀티 스레딩을 적용해봐야 겠다는 생각이 들었다. 

### word cloud의 작동 방법 - 어떤 로직으로 시각화가 이루어지는가?
- 사용자가 generate 함수에 text를 입력하면 generate_from_text 로 넘어가게 되고 여기서 process_text 함수를 호출하게 된다
- process_text 함수는 텍스트를 단어 단위로 나누고, 각 단어의 빈도수를 return으로 전달하는 함수이다.
- 정규식 패턴을 이용하여 단어 리스트를 추출하고 소유격, 숫자, 너무 짧은 단어, 불용어를 제거하게 된다.
- 이후 단어의 빈도수를 세주게 된다. for문과 dictionary를 이용하여 dictionary에 이미 있는 단어라면 count를 올려주고 그렇지 않으면 새로 dictionary를 만들어주게 된다. 이때 단어의 끝에 s혹은 ss가 나오는 것들을 확인하여 복수형도 판단하여 하나로 쳐주게 된다.
- 이때 단어를 소문자로 정규화하고, 같은 단어에 대한 대소문자 차이를 병합. 복수형 단어를 단수형으로 병합하여 중복된 단어의 빈도를 계산하고 각 단어의 등장 빈도수 dictionary와 표준형태를 반환하게 된다.
- 이렇게 반환된 dictionary를 이용하여 word cloud를 그리게 된다.
- word cloud를 그리는 과정은 generate_from_frequencies 함수를 통해 그려진다.
- 해당 함수에서는 빈도수에 따라 글꼴 크기를 선택하고, IntegralOccupancyMap 객체를 활용해 단어를 배치할 위치를 결정한다. 이때 배치가 어려운 경우 글꼴 크기를 변경하거나 방향을 바꾸며 재시도 하게 된다. 
- 이후 단어들을 캔버스에 그리고 이미지를 업데이트하게 된다.


## 회고
### Keep
- M5 팀활동 과정에서 API와 웹 스크래핑 방법 모두를 진행해보면서 데이터의 Extract 과정을 배울 수 있었다. API는 간편하지만 사용량이 제한적이라는 단점이 있었고, 웹 스크래핑 방법은 제한은 없지만 코드를 잘못 짤 경우 너무 오랜시간이 걸리고, ip가 차단될 수 있다는 단점이 있다. 해당 두 방법을 모두 진행해보면서 상황에 맞게 적절히 활용할 수 있을 것 같다. 
- Extract 과정에서 sellinum이 웹 페이지를 열었는데도 데이터가 일정시간이상 뜨지 않아 오류가 발생하는 경우가 많았다. 이를 보완하고, 중단된 지점부터 계속되게 하기 위해 에러가 떠서 종료된다면 그때까지 처리한 작업들을 저장해 놓는 방식으로 구현하여, 중복된 일을 하지 않도록 구현한 방법이 좋았다고 생각한다.

### Problem
- 대용량 데이터를 갖기 위해서는 보통 제한이 없는 웹 스크래핑 방법을 시도할 것 같다. 하지만 오늘처럼 구현한 대로라면 웹 스크래핑을 하는데 매우 오랜시간이 걸릴 것 같다. 단일스레드를 사용하였고, 에러가 종종 발생하였기 때문에 시간이 오래걸렸다. 
- 아직 1000개 정도의 데이터에서도 많은 시간이 발생하게 되는데, 대용량 데이터를 다루거나, 추출해내야 할 경우가 근심이다.

### Try
- 멀티스레드 환경을 Extract에 적용시켜서 시간을 단축할 수 있도록 해야겠다. 또 에러처리를 고도화하여 에러 발생 후 다시 재시도 하는 과정을 구현해서 한 번 시작하면 끝까지 원하는 데이터를 모두 추출해올 수 있도록 구현해야겠다. 이를 통해 대용량 데이터를 통해 빠르고 안정적인 Extract 함수를 만들고 싶다.