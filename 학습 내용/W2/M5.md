# W2M5 학습내용 정리

## 팀활동
### 제공된 데이터셋과 유사한 데이터셋을 웹 스크레이핑을 통해 만든 다음, word cloud를 만들어 봅시다.
- 유튜브 잇섭 채널의 동영상 조회수와 좋아요 수를 활용하여 채널 내에서 조회수가 높은 컨텐츠들의 제목에는 어떤 키워드들이 들어있는지, 또 채널 내에서 조회수가 낮은 컨텐츠에는 어떤 키워드들이 들어있는지 word cloud를 만들어보았다. 데이터셋을 추출하는 과정에는 유튜브 API를 사용하는 과정과 sellenium을 이용하여 웹 스크래핑 하는 과정 모두 구현하였다. API를 사용하여 비교적 쉽게 얻을 수 있는 데이터들이 직접 웹 스크래핑을 통해서는 추출해내는데 오랜 시간이 걸렸다. 하지만 웹 스크래핑의 경우 일일 한도량이 없다는 장점이 있었다. 상황에 맞게 적절히 활용하여 데이터를 수집할 수 있어야 한다. 웹 스크래핑을 통한 Extract 과정에서는 웹페이지 혹은 네트워크 오류등으로 오류가 발생하는 경우가 종종 있었다. 따라서 이를 보완하기 위해, 중단된 지점까지는 저장하고, 다음에 실행할 때에는 중단된 지점 후부터 추출할 수 있는 방식으로 구현하였다. 
- 유튜브 조회수, 좋아요 수, 댓글 수의 상관관계를 파악하여 조회수와 좋아요 수는 상관관계가 0.7로 높은 편임을 확인할 수 있었다. 이를 바탕으로 선호 컨텐츠와 비선호 컨텐츠를 정할 때 조회수와 좋아요 수를 사용하여 판단하였다.
- 조회수, 좋아요 수 모두 상위 25%인 영상을 선호 컨텐츠, 조회수와 좋아요 수 모두 하위 25% 인 영상을 비선호 컨텐츠로 정의하였다. 
- 실험 결과 해당 크리에이터의 영상 주제가 비슷하다보니 조회수가 많은 word cloud와 적은 word cloud가 겹치는게 많은 것을 확인할 수 있었다. 그래도 조회수가 많은 컨텐츠로는 삼성 갤럭시 울트라, 키보드, 폴드, 일주일 사용기와 같은 키워드들이 눈에 띄게 보이는 것을 확인할 수 있었다.
- 추출한 데이터의 전처리 과정을 추가하여 보완할 수 있을 것 같다.

### 데이터셋을 만들 때 어떤 작업들이 추가적으로 필요할까요?
- 웹 스크래핑을 통해서 만들 때는 sellenium을 이용하여 데이터를 추출해오고, 필요한 데이터들을 선별해 저장하는 방식이 필요하였다. 또 가끔 데이터가 없는 부분도 있었기에 이에 대한 에러처리도 필요했다. API를 활용하여 데이터셋을 만들때는 원하는 데이터를 API가 제공하는지 확인하고, 최대 호출 한도를 넘지 않게 하기 위해 적절히 API를 호출할 수 있도록 구조를 짜는 것이 중요했다. 
- 이후 원하는 통계 데이터를 찾기 위해 상관계수를 구하는 corr 메소드, 각종 통계를 볼 수 있는 describe 메소드를 추가적으로 활용해 통계적 데이터들을 추가해줄 수 있었다. 

### 해당 분석을 통해 어떤 비즈니스 가치를 만들 수 있을지에 대해 토의합시다
- wordCloud를 통해서 크리에이터는 구독자 혹은 영상 시청자들이 자신의 채널에서 기대하는 혹은 선호하는(조회수가 많거나, 좋아요수가 많은) 영상의 주제, 키워드를 파악할 수 있다. 이를 통해 영상 주제 선택 혹은 채널의 방향성에 대해 파악할 수 있을 것이다. 

## 추가 요구 사항

### word cloud의 작동 방법 - 어떤 로직으로 시각화가 이루어지는가?

- 사용자가 generate 함수에 text를 입력하면 generate_from_text 로 넘어가게 되고 여기서 process_text 함수를 호출하게 된다
- process_text 함수는 텍스트를 단어 단위로 나누고, 각 단어의 빈도수를 return으로 전달하는 함수이다.
- 정규식 패턴을 이용하여 단어 리스트를 추출하고 소유격, 숫자, 너무 짧은 단어, 불용어를 제거하게 된다.
- 이후 단어의 빈도수를 세주게 된다. for문과 dictionary를 이용하여 dictionary에 이미 있는 단어라면 count를 올려주고 그렇지 않으면 새로 dictionary를 만들어주게 된다. 이때 단어의 끝에 s혹은 ss가 나오는 것들을 확인하여 복수형도 판단하여 하나로 쳐주게 된다.
이때 단어를 소문자로 정규화하고, 같은 단어에 대한 대소문자 차이를 병합. 복수형 단어를 단수형으로 병합하여 중복된 단어의 빈도를 계산하고 각 단어의 등장 빈도수 dictionary와 표준형태를 반환하게 된다.
- 이렇게 반환된 dictionary를 이용하여 word cloud를 그리게 된다.
word cloud를 그리는 과정은 generate_from_frequencies 함수를 통해 그려진다.
- 해당 함수에서는 빈도수에 따라 글꼴 크기를 선택하고, IntegralOccupancyMap 객체를 활용해 단어를 배치할 위치를 결정한다. 이때 배치가 어려운 경우 글꼴 크기를 변경하거나 방향을 바꾸며 재시도 하게 된다.
- 이후 단어들을 캔버스에 그리고 이미지를 업데이트하게 된다.

### Predicting Election Results from Twitter Using Machine Learning Algorithms Summary 읽고 소감
- 소셜미디어에는 다양한 뉴스와 이벤트에 대한 정보를 포함하고 있다. 이러한 정보들을 토대로 선거 결과를 예측하는 연구 주제가 떠오르고 있다. 논문에서는 소셜 미디어 사용자들의 정치적 성향을 분석하고 시각화 하기 위해, micro blog에서 하위 이벤트 발견 및 감성 분석을 결합하여 선거 결과를 예측하는 전략을 제안하고 있다.
- 본 논문의 모델에서는 트위터 데이터를 활용하여 선거 결과를 예측한다. 트위터 정보를 수집해 후보자들에 대한 감정을 분석함으로써 선거 결과를 예측한다.
- 15 fold cross validation이 적용된 SVM 방식을 활용하여 감정 극성 분류 및 각 정당의 지지점수를 계산해, 이를 기반으로 정당별 의석 수를 예측, 94.2 % 정확도의 선거 실제 결과 예측을 이뤘다. 
- 기존 조사 방식인 출구 조사 같은 방식과 비교했을 때 소셜 미디어 데이터가 더 높은 정확도로 결과를 예측할 수 있다.
- 더 트위터가 활발한 지역 및 국가로 해당 연구를 확장할 수 있으며, 선거 외에도 국가 기관 등 다양한 분야로 연구를 확장할 수 있다. 
- 트윗과 같은 비구조적 데이터를 어떻게 활용하느냐에 따라 유의미한 정보를 도출할 수 있다는 것을 보여주는 논문인 것 같다. 이는 선거 예측 뿐만 아니라, 다양한 마케팅, 정책 등 사람들의 의견 혹은 선호가 반영되는 분야에서 활용될 수 있을 것이다. 또 실시간으로 많은 양의 데이터들이 만들어지고 있기 때문에, 빠르게 변하는 트렌드를 반영해 즉각적인 분석과 대응이 가능할 것이다. 이와 같은 데이터들에서 인사이트를 발견하고, 비즈니스에 활용할 수 있는 Data Product를 만들어내는 것이 중요하다는 생각이 들었다. 이외에도 소셜 미디어에는 허위 정보나 자동화 된 봇 계정이 존재하기 때문에, 이러한 요소들에 대한 데이터를 선별하고 정제하는 과정도 중요하다라는 생각이 들었다.