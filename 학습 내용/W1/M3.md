# W1M3 학습내용 정리

## 팀활동
### wikipeida 페이지가 아닌, IMF 홈페이지에서 직접 데이터를 가져오는 방법은 없을까요? 어떻게 하면 될까요?
- IMF API를 사용하여 각 나라별 GDP 를 가져오고, Region 별로 GDP 평균을 구해오는 API를 사용하여 구할 수 있다.

```python
def getIMFGdpByCountry(period = 2025):
    url = "https://www.imf.org/external/datamapper/api/v1/NGDPD?periods={pr}".format(pr = period)
    response = requests.get(url)
    raw_data = []
    
    if response.status_code == 200:
        gdp_data = response.json()
        for country, data in gdp_data["values"]["NGDPD"].items():
            for year, gdp in data.items():
                raw_data.append({"Country": country, "Year" : int(year), "GDP" : gdp})

    df = pd.DataFrame(raw_data)
    return df
        
def getIMFGdpByRegion():
    region_code_url = "https://www.imf.org/external/datamapper/api/v1/regions"
    response = requests.get(region_code_url)
    code_raw_data = dict()

    if response.status_code == 200:
        code_data = response.json()
        for code, data in code_data["regions"].items():
            code_raw_data[code] = data["label"]

    raw_data = []

    region_code_url = "https://www.imf.org/external/datamapper/api/v1/NGDPD?periods=2025".format(RCODE = code)
    response = requests.get(region_code_url)

    if response.status_code == 200:
        gdp_data = response.json()

        if "values" in gdp_data :
            for region, data in gdp_data["values"]["NGDPD"].items():
                if region in code_raw_data.keys():
                    for year, gdp in data.items():
                        raw_data.append({"Region": code_raw_data[region], "Year" : int(year), "GDP" : gdp})
        
    df = pd.DataFrame(raw_data)
    return df
```

- 하지만, IMF 웹사이트에서는 각 나라별로 Region을 어떤식으로 Mapping 하는지에 대한 데이터를 제공하지 않고 있어서, 데이터베이스에 각 나라별 Region을 저장할 수는 없었다.


### 만약 데이터가 갱신되면 과거의 데이터는 어떻게 되어야 할까요? 과거의 데이터를 조회하는 게 필요하다면 ETL 프로세스를 어떻게 변경해야 할까요?

- ETL 프로세스의 Extract 과정에서는 데이터를 저장할 때 날짜도 파일 명에 저장하도록 하여, 중복해서 겹치지 않게 저장하도록 한다. 이와 마찬가지로 Load 과정에서도 날짜를 파일 명에 저장하도록 하여, 데이터를 덮어 씌우지 않고, 과거의 데이터를 보존하도록 한다.
- ETL 전체 과정을 IMF 데이터가 업데이트 되는 주기에 맞춰 ETL 과정이 이뤄지도록 구현한다. 매년 2회 자료가 제공되기 때문에 6개월 간격으로 ETL 이 진행되어 6개월 전 데이터가 저장되어 있을 경우 ETL을 진행하고, 6개월 이내의 데이터라면 이미 추출한 데이터를 보여주는 방식으로 구현한다.
- 위 방법에서는 로그를 활용할 수 있다. 로그 상에서 6개월 이내에 데이터를 추출하였고, 그 데이터 파일명을 통해 가장 최신의 정보라면 이미 추출한 데이터를 사용하고, 가장 최신의 정보가 아니라면 ETL이 진행되어 새로운 데이터를 저장하도록 한다.

## 새롭게 알게 된 내용

### BeautifulSoup 사용법
- url의 html을 받아와 soup 객체로 변환
- html 요소를 찾아 활용하기
- beatifulsoup4는 자바스크립트로 동적으로 생성된 정보는 가져올 수 없어서, selenium 라이브러리를 사용한다.
- select 메소드
    - css 선택자를 활용해서 HTML 태그를 찾는 방식
``` python
    titles = soup.select("div.cont_thumb > p.txt_thumb")
    for title in titles:
        if title is not None:
            print(title.text)
```
- find 메소드
``` python
    cont_thumb = soup.find_all("div", "cont_thumb")
    for cont in cont_thumb:
        title = cont.find("p", "txt_thumb")
        if title is not None:
            print(title.text)
```

### ETL
- Extract
    - 다양한 데이터 소스로부터 데이터를 추출하는 과정. 원본 형태를 유지하는 것이 중요하며, 무결성을 보장해야 한다.
    - 미션에서는 BeautifulSoup를 이용하여 웹에서 데이터를 추출해왔다.
- Transform 
    - 추출된 데이터를 비즈니스 규칙이나 요구 사항에 맞게 변환 하는 과정.
    - 미션에서는 dataframe을 활용하여 데이터 요구에 맞게 가공해주었다.
- Load
    - 변환된 데이터를 최종적으로 데이터 웨어하우스 같은 저장소에 적재하는 과정.
    - 미션에서는 sqlite3를 이용하여 transform 해준 데이터를 db 파일로 저장하였다.

### Pandas 배운점
- apply 함수
    - 데이터 셋에 내가 구현한 연산을 시행하거나 변환해야 할 때 사용
    - lambda를 사용하거나, 새로운 함수를 파라미터로 추가해준다.
- group by
    - dataframe에서도 groupby 기능을 제공해준다.

### SQL
- Partition By 
    - group by와 유사하지만 partition by를 사용하면, 기존의 세세한 정보들이 사라지지 않고 그대로 유지된다. 즉, 기존의 데이터가 하나로 겹쳐지지 않는다. 

- ROW_NUM
    - 1부터 순서대로 순번을 매기게 된다.

### MapReduce
- 분산 컴퓨팅의 대표적인 프레임워크로, 대량의 데이터를 병렬로 처리하는데 사용되는 방법. Map 단계와 Reduce 단계로 구성. 
- Map phase : 제일 먼저 수행. 데이터의 여러 파티션에 병렬 분산으로 호출되어 수행. 맵 함수는 (key, value) 쌍 형태로 결과를 출력하고, 여러 머신에 나누어 보내며 같은 key를 가진 key, value 쌍은 같은 머신으로 보내진다.
    - input : (key1, value1)
    - output : list(key2, value2)
- Shuffling Phase : 각각의 머신으로 보내진 key,value 쌍의 key를 이용해 정렬한 후에 각각의 key마다 같은 Key를 가진(key, value) 쌍을 모아서 (key,list(value))로 만든 다음에 key에 따라 여러 머신에 분산해서 보낸다.
    - input : list(key2, value2)
    - output : (key2, list(value2))
- Reduce Phase : 모든 머신에서 셔플링 페이즈가 끝나면 각 머신마다 리듀스 페이즈가 시작.  각각의 머신에서는 (key, list(value)) 쌍 마다 리듀스 함수가 호출되며 출력으로 key, value 쌍을 내보내고 다음 쌍 시작. 맵화한 작업 중 중복 데이터를 제거하고 원하는 데이터를 추출하는 작업. (최종 결과 생성)
    - input : (key2, list(value2))
    - output : list(key3,value3)



## 생각해본 것들

### Q.고민해 볼 것은 Extract한 데이터를 바로 처리 (Transform)하지 않고 저장하는 이유가 뭘까요? 어떤 경우에 그렇게 해야 할까요?
- 데이터를 어떻게 바라보느냐에 따라 데이터의 type이 다르게 보인다고 할 수 있다. 이에 따라 transform을 하게 된 데이터는 사용자가 데이터를 어떻게 바라봐야할 것인지 결정을 하고 변환을 하게 된다. transform이 된 데이터는 어떻게 사용할지가 정해진 데이터라고 할 수 있다. 이처럼 데이터의 사용이 정해지지 않았을 때 추후 사용을 위해 Raw data를 저장한다.
- Structed Data의 경우 Queue의 함정에 빠질 수 있다. 주어진 틀에 맞춰 생각을 하게 되고, 사용되게 된다. 이처럼 Queue에 함정에 빠지지 않기 위해 Raw data를 저장한다.
- Extract한 데이터가 매우 클때, Transform 전에 저장하지 않는다면 Transform 과정에서 오류가 발생하였을 때 롤백을 하기 힘들 뿐만 아니라, 다시 실행하게 될 때에도 extract과정을 다시 거쳐야 하는 불편함이 있을 수 있다. 따라서 transform 전에 원시 데이터를 저장하는 과정을 거쳐야 한다.
- 여러 소스 시스템에서의 데이터를 통합 할 수 있다. 혹은 병렬 처리를 할 때 한 번에 데이터를 뽑아와 rawData를 저장시켜 놓는다. 

#### 고민사항
미션에서 Extract한 데이터를 Transform 하지 않고, Raw data 상태에서 json 파일로 저장하는 부분이 있다. 미션 과정의 Extract에서 위키피디아 url을 통해 html을 불러온 후 beautifulsoup를 이용해서 GDP 테이블에서 나라이름, GDP, Year을 찾아 배열에 넣은 후 이를 json 으로 저장하고 있다. 하지만 이러한 과정이 진정한 raw_data를 저장하고 있는지에 대한 의구심이 생겼다. raw_data를 저장하는 이유는 정형화된 데이터의 Queue의 함정에 빠지지 않기 위해라는 생각을 하였는데, html에서 beautifulsoup을 이용해 데이터를 extract하는 과정 속에서 이미 원하는 컬럼을 정해 뽑는 과정이 정형화를 하고 있는 것 아닐까 라는 생각이 들었다.

### Q.Raw 데이터의 양이 압도적으로 많다면?
Extract 과정 개선
- 기존에 구현한 웹에서 스크래핑하는 과정에서는 html 파일을 받아오는 과정에서 request.get() 메소드를 사용하고 있었다.
- request.get() 메소드의 기본값은 stream = False 이다.
- 해당 방법은 서버로부터 받은 응답 데이터를 한꺼번에 메모리에 로드하게 된다.
- 이로 인해 대용량 파일 다운로드 일경우, 메모리 과부하 발생 가능하다.
- stream = True 옵션을 사용하여 response를 청크 단위로 쪼개어 메모리에 로드할 수 있도록 개선하였다.
``` python
    with requests.get(url, stream = True) as response:
        response.raise_for_status()
        with open('file.html','wb') as file :
            for chunk in response.iter_content(chunk_size = 8192):
                if chunk:
                    file.write(chunk)
        
    with open('file.html','rb') as file:
        html = file.read()
```

한계
- 미션에서 사용한 판다스는 Numpy 기반이기 때문에 데이터 처리 속도가 빠르다. 판다스의 데이터 프레임은 메모리(Ram)상에서 전개된다. in-memory 방식이 채택되었기 때문에 속도가 빠르다. 판다스는 데이터를 한 번에 메모리에 올려 eager 하다. 하지만 이로인해 데이터의 용량이 커져버리면 out-of-memory가 뜨게 된다. 대용량 데이터라면 out of memory가 뜨기 때문에 eager한 실행을 하는 pandas 대신 lazy한 실행이 이뤄지는 PySpark를 사용해야한다.

### Q.Raw 데이터를 Transform 하는데 시간이 아주 오래 걸린다면?
Transform 과정 개선
- 병렬/분산 처리를 추가하였다.
- 기존의 함수 구조에서는 transform 과정에서 json 파일을 불러와서 transform을 진행하였는데 병렬 처리를 위해서 json 파일을 불러오는 과정을 바깥으로 빼고 transform 메소드는 transform만 담당하도록 구현하였다.
- 파이썬의 멀티프로세싱 라이브러리 Pool 이용해서 병렬처리 구현하였다. 주피터 환경에서는 실행안되고 python으로 실행시켜야했다.
- 단순히 해당 프로젝트에서는 데이터 row 수가 200개 밖에 안되었기 때문에 system call 호출 시간등으로 인해 해당 함수로 transform을 하면 더욱 느렸지만, 데이터 수가 증가할 수록 효과가 좋을 것 같다.

``` python
    # df에는 데이터셋, func에는 transform 함수를 전달한다.
    def parallel_dataframe(df, func, num_cores = 8):
        df_split = np.array_split(df, num_cores)
        pool = Pool(num_cores)
        df = pd.concat(pool.map(func, df_split))
        pool.close()
        pool.join()
        return df
```

### Q. 여러분이 작성한 코드를 ETL로 잘 나눠 보세요. 정의대로 잘 나눠지나요?
여러분이 작성한 코드는 ETL 각각의 단계가 독립적으로 구분되어 작동하나요?
- Extract, Transform,Load 모두 각각의 입력을 받을 수 있고, 출력이 이뤄지도록 구현하여 각 단계가 독립적으로 구분되어 작동되고 있다. 하지만 어디까지가 Extract인지, Transform인지에 대한 경계가 애매모호 하다는 생각이 들기도 하였다. 또 Transform 내에서도 에러 안정성을 위해 분리를 해야 한다고 생각해서 분리했는데, 어느 정도 처리까지 분리를 해야할지 애매한 부분도 있었다.

만약 ETL에서 각각의 과정을 별도로 프로세스로 처리한다면 어떻게 코드를 변경해야 할까요?
- 각각의 input과 output을 받을 수 있는 함수로 구현하여 서로 의존하지 않고, 독립적으로 실행될 수 있도록 함수를 만들어야 한다고 생각한다. 

병렬/분산 처리를 한다면 대응 가능한 코드인가요?
- 병렬/분산 처리에 대응이 가능하도록 파이썬 Pool 라이브러리를 이용하여 구현하였다. Transform 내부에서 분산 처리가 이뤄지는데 이 과정은 순서에 의미가 없어서 단순히 인덱스 순서대로 분할하여 GDP 단위 변경과, Region 매핑을 진행해 줄 수 있었다. 

대용량 데이터를 처리하는 과정에서 에러가 난다면 어떻게 해야 할까요?
- checkpoint를 활용하여 에러가 난 시점 혹은 batch를 로그에 기록하여 두고, 해당 시점 부터 다시 처리를 진행한다. 데이터베이스에서 트랜잭션을 롤백하고, 다시 시도하는 로직처럼 에러가 발생했을 때 복구할 수 있는 시나리오 등을 미리 준비해두어야 할 것 같다. 또 에러가 나지 않기 위해서 적절한 메모리 사용을 유의한 코딩이 필요하다고 생각한다. 